{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "85pErh9t2koS",
    "outputId": "c6a03f7f-0c66-48ad-e5d9-418a4d83e0d9"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "5EfmxmAh2qXt",
    "outputId": "e8dbae02-ce30-45ce-b9c6-127d133aeda6"
   },
   "outputs": [],
   "source": [
    "# #Making the current directory as the directory where all the files are.\n",
    "# % cd ./drive/\"My Drive\"/hw4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "0P55ReZRqIWg",
    "outputId": "d66fe417-7afb-4ae1-9ab6-ac61c392f925"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.utils as utils\n",
    "import pickle as pk\n",
    "from torch.utils.data import DataLoader, Dataset \n",
    "import time\n",
    "from torch.nn.utils.rnn import *\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "KQ3d1clDqOJ7",
    "outputId": "0388c9ad-b7eb-4088-8712-34f9ce9f2e53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loading Sucessful.....\n"
     ]
    }
   ],
   "source": [
    "speech_train = np.load('train_new.npy', allow_pickle=True, encoding='bytes')\n",
    "speech_valid = np.load('dev_new.npy', allow_pickle=True, encoding='bytes')\n",
    "speech_test = np.load('test_new.npy', allow_pickle=True, encoding='bytes')\n",
    "\n",
    "transcript_train = np.load('./train_transcripts.npy', allow_pickle=True,encoding='bytes')\n",
    "transcript_valid = np.load('./dev_transcripts.npy', allow_pickle=True,encoding='bytes')\n",
    "print(\"Data Loading Sucessful.....\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u6zLB6wxvLuu"
   },
   "outputs": [],
   "source": [
    "letter_list = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q',\\\n",
    "             'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '-', \"'\", '.', '_', '+', ' ','<sos>','<eos>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eIs4yM6cxjsp"
   },
   "outputs": [],
   "source": [
    "letter_dict = {}\n",
    "k = 1\n",
    "for l in letter_list:\n",
    "    letter_dict[l] = k\n",
    "    k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4nvT4PIfPV5E"
   },
   "outputs": [],
   "source": [
    "def transform_letter_to_index(text, letter_dict):\n",
    "    \n",
    "    '''\n",
    "    :param transcript :(N, ) Transcripts are the text input\n",
    "    :param letter_list: Letter list defined above\n",
    "    :return letter_to_index_list: Returns a list for all the transcript sentence to index\n",
    "    '''\n",
    "    \n",
    "    out = []\n",
    "\n",
    "    for pos, line in enumerate(text):\n",
    "        \n",
    "        if len(line) > 0:\n",
    "            line = line.astype(str)            \n",
    "            chrs = \" \".join(o for o in line)\n",
    "            A = [letter_dict[c] for c in chrs]\n",
    "            A.insert(0, letter_dict['<sos>'])\n",
    "            A.append(letter_dict['<eos>'])\n",
    "            out.append(np.array(A))\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "6UQQe-VovbNs",
    "outputId": "3d7d3270-0c36-4b79-a4b3-09d8b2338d08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed data sucessfully.....\n"
     ]
    }
   ],
   "source": [
    "character_text_train = transform_letter_to_index(transcript_train, letter_dict)\n",
    "character_text_valid = transform_letter_to_index(transcript_valid, letter_dict)\n",
    "print(\"Transformed data sucessfully.....\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KV_KYWyA7Yds"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([33, 20,  8,  5, 32,  6,  5, 13,  1, 12,  5, 32, 16, 18, 15,  4, 21,\n",
       "        3,  5, 19, 32,  1, 32, 12,  9, 20, 20,  5, 18, 32, 15,  6, 32, 20,\n",
       "       23, 15, 32, 20, 15, 32,  6, 15, 21, 18, 32, 25, 15, 21, 14,  7, 32,\n",
       "        9, 14, 32, 14, 15, 22,  5, 13,  2,  5, 18, 32,  1, 14,  4, 32,  4,\n",
       "        5,  3,  5, 13,  2,  5, 18, 34])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(character_text_train)\n",
    "character_text_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DJZRV0QStepp"
   },
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DKBfSCXptjuG"
   },
   "outputs": [],
   "source": [
    "class Speech2Text_Dataset(Dataset):\n",
    "    def __init__(self, speech, text=None, train=True):\n",
    "        self.speech = speech\n",
    "        self.train = train\n",
    "        \n",
    "        if(text is not None):\n",
    "            self.text = text\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.speech.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        if(self.train):\n",
    "            return torch.tensor(self.speech[index].astype(np.float32)), torch.tensor(self.text[index])\n",
    "        else:\n",
    "            return torch.tensor(self.speech[index].astype(np.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y4vFamVttyt_"
   },
   "outputs": [],
   "source": [
    "def collate_train(batch_data):\n",
    "    \n",
    "    Frame = []\n",
    "    Text = []\n",
    "    frame_len = []\n",
    "    text_len = []\n",
    "\n",
    "    for i, (frames, text) in enumerate(batch_data):\n",
    "        Frame.append(frames)\n",
    "        Text.append(text)\n",
    "        frame_len.append(frames.size(0))\n",
    "        text_len.append(text.size(0))\n",
    "\n",
    "    all_frames = pad_sequence(Frame, batch_first = False)\n",
    "    all_text = pad_sequence(Text, batch_first = True)\n",
    "\n",
    "    return all_frames, frame_len, all_text, text_len\n",
    "\n",
    "\n",
    "def collate_test(batch_data):\n",
    "    \n",
    "    Frame = []\n",
    "    frame_len = []\n",
    "\n",
    "    for i, (frames) in enumerate(batch_data):\n",
    "        Frame.append(frames)\n",
    "        frame_len.append(frames.size(0))\n",
    "\n",
    "    all_frames = pad_sequence(Frame, batch_first = False)\n",
    "\n",
    "    return all_frames, frame_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XFSy8Lgnt_b0"
   },
   "outputs": [],
   "source": [
    "Speech2Text_train_Dataset = Speech2Text_Dataset(speech = speech_train, text = character_text_train)\n",
    "Speech2Text_valid_Dataset = Speech2Text_Dataset(speech = speech_valid, text = character_text_valid)\n",
    "Speech2Text_test_Dataset = Speech2Text_Dataset(speech = speech_test, text = None, train = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IVllkAsOwvmB"
   },
   "outputs": [],
   "source": [
    "# data loader\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        Speech2Text_train_Dataset,\n",
    "        batch_size=64, shuffle=True, collate_fn=collate_train)\n",
    "\n",
    "dev_loader = torch.utils.data.DataLoader(\n",
    "        Speech2Text_valid_Dataset,\n",
    "        batch_size=64, shuffle= True, collate_fn=collate_train)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        Speech2Text_test_Dataset,\n",
    "        batch_size=1, shuffle= False, collate_fn=collate_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "PW_Z7bZtwisU",
    "outputId": "11da34ab-6c63-4297-8ce3-92027ceb86c8"
   },
   "outputs": [],
   "source": [
    "# k = 0\n",
    "\n",
    "# for batch, (frames, seq_sizes, _, _) in enumerate(dev_loader):\n",
    "#     if(k < 5):\n",
    "#         print(frames.shape)\n",
    "# #         print(seq_sizes)\n",
    "#         k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropouts\n",
    "\n",
    "class LockedDrop(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x, dropout=0.5):\n",
    "        if not self.training or not dropout:\n",
    "            return x\n",
    "        m = x.data.new(1, x.size(1), x.size(2)).bernoulli_(1 - dropout)\n",
    "        mask = Variable(m, requires_grad=False) / (1 - dropout)\n",
    "        mask = mask.expand_as(x)\n",
    "        return mask * x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GLdLTiVmvSFm"
   },
   "source": [
    "# Pyramidal BLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r_xwg8y4uy4K"
   },
   "outputs": [],
   "source": [
    "class pBLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(pBLSTM, self).__init__()\n",
    "        self.blstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim, num_layers=1, bidirectional=True)\n",
    "        self.drop = LockedDrop()\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        :param x :(N,T) input to the pBLSTM\n",
    "        :return output: (N,T,H) encoded sequence from pyramidal Bi-LSTM \n",
    "        '''\n",
    "        # x is output of lstm layer and is a packed sequence\n",
    "        \n",
    "        pad, lengths = pad_packed_sequence(x, batch_first=True)\n",
    "        if pad.size(1) % 2 > 0:\n",
    "            # if odd, remove last frame\n",
    "            pad = pad[:, :-1, :]\n",
    "            \n",
    "        h, k, l = np.shape(pad)\n",
    "        pad = pad.view(h, k//2, l*2)\n",
    "\n",
    "        new_len = lengths//2 \n",
    "        \n",
    "        x = self.drop.forward(pad, dropout = 0.15)\n",
    "        \n",
    "        input_x = pack_padded_sequence(x, new_len, enforce_sorted = False, batch_first=True)\n",
    "\n",
    "        output, hidden = self.blstm(input_x)\n",
    "        \n",
    "        return output, hidden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M2L8aIpC1iLv"
   },
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ftU398GQvx0j"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, value_size=128,key_size=128):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=input_dim,hidden_size=hidden_dim,num_layers=1,bidirectional=True)\n",
    "        \n",
    "        # Here you need to define the blocks of pBLSTMs\n",
    "\n",
    "        # output from previous lstm is 2*hidden_dim and after reshaping, it becomes 4*hidden_dim\n",
    "        self.plstm1 = pBLSTM(4*hidden_dim, hidden_dim)\n",
    "        self.plstm2 = pBLSTM(4*hidden_dim, hidden_dim)\n",
    "        self.plstm3 = pBLSTM(4*hidden_dim, hidden_dim)\n",
    "\n",
    "        self.key_network = nn.Linear(hidden_dim*2, value_size)\n",
    "        self.value_network = nn.Linear(hidden_dim*2, key_size)\n",
    "\n",
    "    def forward(self, x, lens):\n",
    "        \n",
    "         # x is already padded\n",
    "\n",
    "        rnn_inp = pack_padded_sequence(x, lengths=lens, batch_first=False, enforce_sorted=False)\n",
    "        outputs = self.lstm(rnn_inp)[0]\n",
    "        \n",
    "        # dimension of outputs is 2*hidden_dim and outputs is packed sequence\n",
    "        outputs, _ = self.plstm1(outputs)\n",
    "        outputs, _ = self.plstm2(outputs)\n",
    "        outputs, _ = self.plstm3(outputs)\n",
    "\n",
    "        linear_input, lens = pad_packed_sequence(outputs)\n",
    "        \n",
    "        keys = self.key_network(linear_input)\n",
    "        value = self.value_network(linear_input)\n",
    "\n",
    "        return keys, value, lens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lcjyTREEPLMg"
   },
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Azk9irRiPIbh"
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Attention, self).__init__()\n",
    "    \n",
    "    def forward(self, query, key, values, lengths):\n",
    "        \n",
    "        \"\"\"\n",
    "        :param query: (N, H), decoder state of a single timestep\n",
    "        :param key: (N, T, K), key from encoder\n",
    "        :param context: (N, T, H), encoded source sequences\n",
    "        :param lengths: (N,), lengths of source sequences\n",
    "        :returns: (N, H) attended source context, and (N, T) attention vectors\n",
    "        \"\"\"\n",
    "        \n",
    "        # print(\"ATTENTION!!!!\")\n",
    "        # print(np.shape(query), key.shape, value.shape)\n",
    "        \n",
    "        attention = torch.bmm(key, query.unsqueeze(2)).squeeze(2)\n",
    "  \n",
    "        mask = torch.arange(key.size(1)).unsqueeze(0) >= lengths.unsqueeze(1)\n",
    "        \n",
    "        attention.masked_fill_(mask.to(device), -1e9)\n",
    "        attention = nn.functional.softmax(attention, dim=1)\n",
    "        out = torch.bmm(attention.unsqueeze(1), values).squeeze(1)\n",
    "        \n",
    "        return out, attention\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0V76R3sBQOAK"
   },
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gmI8Jj92DCqV"
   },
   "outputs": [],
   "source": [
    "from torch.distributions.gumbel import Gumbel \n",
    "# np.random.rand??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction = torch.tensor(np.random.rand(1, 35))\n",
    "# print(prediction.shape)\n",
    "# print(prediction)\n",
    "# noise_prediction = Gumbel(prediction, torch.FloatTensor([0.1])).sample()\n",
    "# print(noise_prediction)\n",
    "\n",
    "# print(noise_prediction - prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uUKD2z0RQKbD"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, hidden_dim, value_size=128, key_size=128,  isAttended=True):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_dim)\n",
    "\n",
    "        self.lstm1 = nn.LSTMCell(input_size=hidden_dim+value_size, hidden_size=hidden_dim)\n",
    "        self.lstm2 = nn.LSTMCell(input_size=hidden_dim, hidden_size=key_size)\n",
    "\n",
    "        self.isAttended = isAttended\n",
    "        \n",
    "        if(isAttended):\n",
    "            self.attention = Attention()\n",
    "        \n",
    "        self.character_prob = nn.Linear(key_size+value_size, vocab_size)\n",
    "            \n",
    "\n",
    "    def forward(self, key, values, text=None, train=True, lens = None, gumbel_noise = 0.1, tf = 0.9):\n",
    "        \n",
    "        '''\n",
    "        :param key :(T, N, key_size) Output of the Encoder Key projection layer\n",
    "        :param values: (T, N, value_size) Output of the Encoder Value projection layer\n",
    "        :param text: (N, text_len) Batch input of text with text_length\n",
    "        :param train: Train or eval mode\n",
    "        :lens: actual length for input text\n",
    "        :return predictions: Returns the character perdiction probability \n",
    "        '''\n",
    "        \n",
    "        batch_size = key.shape[1]\n",
    "        \n",
    "        if(train):\n",
    "            max_len =  text.shape[1] - 1\n",
    "            embeddings = self.embedding(text)\n",
    "            \n",
    "        else:\n",
    "            max_len = 250\n",
    "            \n",
    "        predictions = []\n",
    "\n",
    "        hidden_states = [None, None]\n",
    "        prediction = torch.zeros(batch_size, 1).to(device)\n",
    "        \n",
    "        # taking zero context for first character\n",
    "#         context = torch.zeros(batch_size, 128).to(device)\n",
    "#         gumble_noise = 0.4\n",
    "        # print(\"max_len: \", max_len)\n",
    "    \n",
    "        context = values[0, :, :]\n",
    "\n",
    "        for i in range(max_len):\n",
    "            \n",
    "            '''\n",
    "            Here you should implement Gumble noise and teacher forcing techniques\n",
    "            '''\n",
    "                   \n",
    "            choice = np.random.choice([1, 2], p = [tf, 1-tf])\n",
    "\n",
    "            if(train):\n",
    "\n",
    "                if (choice == 1):\n",
    "                      char_embed = embeddings[:, i, :]\n",
    "\n",
    "                else:\n",
    "#                     noise_prediction = Gumbel(prediction.cpu(), torch.FloatTensor([gumbel_noise])).sample().to(device)                    \n",
    "                    char_embed = self.embedding(prediction.argmax(dim=-1))\n",
    "\n",
    "            else:\n",
    "            # at testing time, simply take the max from last time step\n",
    "                if(i==0):\n",
    "                    # start from sos in test case\n",
    "                    char_embed = self.embedding(prediction.argmax(dim=-1) + 33)\n",
    "                else:\n",
    "                    char_embed = self.embedding(prediction.argmax(dim=-1))\n",
    "#                     noise_prediction = Gumbel(prediction.cpu(), torch.FloatTensor([gumbel_noise])).sample().to(device)\n",
    "#                     char_embed = self.embedding(noise_prediction.argmax(dim=-1))\n",
    "\n",
    "            # print(np.shape(char_embed), np.shape(context))\n",
    "            inp = torch.cat([char_embed, context], dim=1)\n",
    "            hidden_states[0] = self.lstm1(inp, hidden_states[0])\n",
    "\n",
    "            inp_2 = hidden_states[0][0]\n",
    "            hidden_states[1] = self.lstm2(inp_2, hidden_states[1])\n",
    "\n",
    "            output = hidden_states[1][0]\n",
    "            # print(output.shape)\n",
    "            \n",
    "            query = hidden_states[1][0]\n",
    "#             query = query.unsqueeze(1)\n",
    "#             context, _ = self.attention(query, key, values, lens)\n",
    "            context, _ = self.attention(query, key.transpose(0, 1), values.transpose(0, 1), lens)\n",
    "\n",
    "            prediction = self.character_prob(torch.cat([output, context], dim=1)) # batch_size, vocab_size\n",
    "            \n",
    "#             print(\"predictions: \", prediction.shape)\n",
    "#             print(prediction.unsqueeze(1).shape)\n",
    "            predictions.append(prediction.unsqueeze(1))\n",
    "#             print(\"....\", np.shape(predictions))\n",
    "            \n",
    "        return torch.cat(predictions, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rHtinxrtXS9-"
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "\n",
    "    def __init__(self,input_dim,vocab_size,hidden_dim,value_size=128, key_size=128,isAttended=True):\n",
    "        super(Seq2Seq,self).__init__()\n",
    "        self.encoder = Encoder(input_dim, hidden_dim)\n",
    "        self.decoder = Decoder(vocab_size, 2*hidden_dim)\n",
    "\n",
    "    def forward(self,speech_input, speech_len, text_input=None,train=True, tf = 0.9):\n",
    "        key, value, lens = self.encoder(speech_input, speech_len)\n",
    "        \n",
    "        if(train):\n",
    "            predictions = self.decoder(key, value, text_input, lens = lens, tf = tf)\n",
    "        else:\n",
    "            predictions = self.decoder(key, value, text=None, train=False, lens = lens, tf = tf)\n",
    "            \n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "olUTML67cOK2"
   },
   "source": [
    "# Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "heuFkY6FYT4I"
   },
   "outputs": [],
   "source": [
    "model = Seq2Seq(input_dim=40, vocab_size=len(letter_list) + 1, hidden_dim= 256)\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss(reduce=None).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g1jj8SxhcSOw"
   },
   "outputs": [],
   "source": [
    "def train(model, train_loader, num_epochs, criterion, optimizer, dev_loader = None, valid = False):\n",
    "\n",
    "        tf = 0.9\n",
    "        k = 1\n",
    "        for epochs in range(num_epochs):\n",
    "            \n",
    "            if(tf > 0.4 and k > 2):\n",
    "                tf = 0.8*tf\n",
    "                k = 1\n",
    "            else:\n",
    "                k += 1\n",
    "                \n",
    "            if(epochs > 20):\n",
    "                optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "#             if(epochs > 15):\n",
    "#                 optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "                \n",
    "            start = time.time()\n",
    "            \n",
    "            print(\"tf: \", tf)\n",
    "            print(\"epoch: \", epochs + 1)\n",
    "            loss_sum_train = 0\n",
    "            loss_sum_val = 0\n",
    "            \n",
    "            since = time.time()\n",
    "            \n",
    "            model.train()\n",
    "            \n",
    "            for (batch_num, collate_output) in enumerate(train_loader):\n",
    "                \n",
    "                with torch.autograd.set_detect_anomaly(True):\n",
    "\n",
    "                    speech_input, speech_len, text_input, text_len = collate_output\n",
    "                    speech_input = speech_input.to(device)\n",
    "                    speech_len = torch.IntTensor(speech_len)\n",
    "                    text_input = text_input.to(device)\n",
    "                    text_len = torch.IntTensor(text_len)\n",
    "\n",
    "                    predictions = model(speech_input, speech_len ,text_input, tf = tf) #, lens = speech_len)\n",
    "                    mask = torch.zeros(text_input.size()).to(device)\n",
    "\n",
    "                for i in range(len(text_len)):\n",
    "                      mask[i, : text_len[i]] = 1\n",
    "\n",
    "                mask = mask[:, 1:]\n",
    "                mask = mask.reshape(-1).to(device)\n",
    "\n",
    "                text_input2 = text_input[:, 1:]\n",
    "\n",
    "                predictions = predictions.contiguous().view(-1, predictions.size(-1))\n",
    "                text_input2 = text_input2.contiguous().view(-1)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                loss = criterion(predictions, text_input2)\n",
    "                masked_loss = torch.sum(loss*mask)\n",
    "\n",
    "                \n",
    "                batch_size = len(text_len)\n",
    "                total_character = torch.sum(text_len) - batch_size\n",
    "                \n",
    "                ml = masked_loss/total_character\n",
    "                ml.backward()\n",
    "\n",
    "                torch.nn.utils.clip_grad_norm(model.parameters(), 2)\n",
    "                optimizer.step()\n",
    "\n",
    "                current_loss = float(masked_loss.item())/int(torch.sum(mask).item())\n",
    "\n",
    "                if  batch_num % 150 == 1:\n",
    "                      print('train_loss', batch_num, current_loss) \n",
    "                        \n",
    "                loss_sum_train += current_loss\n",
    "                \n",
    "            print(\"train_loss: \", loss_sum_train/batch_num)\n",
    "            \n",
    "            if (valid and epochs% 2 == 0):\n",
    "                \n",
    "                model.eval()\n",
    "                \n",
    "                for (batch_num, collate_output) in enumerate(dev_loader):\n",
    "\n",
    "                    speech_input, speech_len, text_input, text_len = collate_output\n",
    "                    speech_input = speech_input.to(device)\n",
    "                    speech_len = torch.IntTensor(speech_len)\n",
    "                    text_input = text_input.to(device)\n",
    "                    text_len = torch.IntTensor(text_len)\n",
    "\n",
    "                    predictions = model(speech_input, speech_len ,text_input, train = False, tf = tf) #, lens = speech_len)\n",
    "                    mask = torch.zeros(text_input.size()).to(device)\n",
    "\n",
    "                    for i in range(len(text_len)):\n",
    "                          mask[i, : text_len[i]] = 1\n",
    "\n",
    "                    mask = mask[:, 1:]\n",
    "                    mask = mask.reshape(-1).to(device)\n",
    "\n",
    "                    text_input2 = text_input[:, 1:]\n",
    "                    \n",
    "                    max_len = text_input2.shape[1]\n",
    "                    predictions = (predictions[:, :max_len, :]).contiguous().view(-1, predictions.size(-1))\n",
    "                    text_input2 = text_input2.contiguous().view(-1)\n",
    "\n",
    "                    loss = criterion(predictions, text_input2)\n",
    "                    masked_loss = torch.sum(loss*mask)\n",
    "\n",
    "\n",
    "                    batch_size = len(text_len)\n",
    "                    total_character = torch.sum(text_len) - batch_size\n",
    "\n",
    "                    ml = masked_loss/total_character\n",
    "\n",
    "                    current_loss = float(masked_loss.item())/int(torch.sum(mask).item())\n",
    "\n",
    "                    if  batch_num % 25 == 1:\n",
    "                          print('valid_loss', batch_num, current_loss) \n",
    "\n",
    "                    loss_sum_val += current_loss\n",
    "                \n",
    "            print(\"valid_loss: \", loss_sum_val/batch_num)\n",
    "            \n",
    "            print(\"Time: \", time.time() - start)\n",
    "            torch.save(model.state_dict(), \"./run3 \" + str(1 + epochs) + \".pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('./run6 1.pt'))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf:  0.9\n",
      "epoch:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ipykernel_launcher.py:65: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss 1 3.077131984544837\n",
      "train_loss 151 1.1463791797576302\n",
      "train_loss 301 0.8898979302803541\n",
      "train_loss:  1.0823632081334413\n",
      "valid_loss:  0.0\n",
      "Time:  1151.8983986377716\n",
      "tf:  0.9\n",
      "epoch:  2\n",
      "train_loss 1 0.9494495008283821\n",
      "train_loss 151 0.8343695062164709\n",
      "train_loss 301 0.817195171196269\n",
      "train_loss:  0.7980491656319868\n",
      "valid_loss:  0.0\n",
      "Time:  1150.3820641040802\n",
      "tf:  0.7200000000000001\n",
      "epoch:  3\n",
      "train_loss 1 0.8855178592301594\n",
      "train_loss 151 0.9226877273447097\n",
      "train_loss 301 0.8907323673665173\n",
      "train_loss:  0.9636828756048126\n",
      "valid_loss:  0.0\n",
      "Time:  1147.0211157798767\n",
      "tf:  0.7200000000000001\n",
      "epoch:  4\n",
      "train_loss 1 1.0551269896456619\n",
      "train_loss 151 0.9301653832572795\n",
      "train_loss 301 0.7365605446348108\n",
      "train_loss:  0.9052309024068864\n",
      "valid_loss:  0.0\n",
      "Time:  1149.2550394535065\n",
      "tf:  0.7200000000000001\n",
      "epoch:  5\n",
      "train_loss 1 0.7459604658792651\n"
     ]
    }
   ],
   "source": [
    "train(model, train_loader, 30, criterion, optimizer) #, dev_loader = dev_loader, valid = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('./eun2 27.pt'))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tr0asy4NIlDU"
   },
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hY3a8lVsIi23"
   },
   "outputs": [],
   "source": [
    "def test(model, test_loader):\n",
    "\n",
    "\n",
    "    A = []\n",
    "\n",
    "    for (batch_num, collate_output) in enumerate(test_loader):\n",
    "            mul_pred = []\n",
    "            print(\"num\", len(A))\n",
    "            with torch.autograd.set_detect_anomaly(True):\n",
    "\n",
    "                speech_input, speech_len = collate_output\n",
    "                speech_input = speech_input.to(device)\n",
    "                speech_len = torch.IntTensor(speech_len)\n",
    "                prediction = model(speech_input, speech_len, train = False) \n",
    "                \n",
    "#                 for i in range(100):\n",
    "#                     prediction = model(speech_input, speech_len, train = False) \n",
    "#                     mul_pred.append(prediction)\n",
    "                    \n",
    "#                 avg_pred = np.log(mul_pred)             \n",
    "                \n",
    "                predictions = prediction.argmax(dim=-1)\n",
    "                A.append(predictions)\n",
    "            break\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_len(prob):\n",
    "    # 100, 250, 35\n",
    "    # find the actual length of these 100 sentences\n",
    "    A  = np.argmax(prob, axis = -1)\n",
    "    lengths = np.zeros(len(A))\n",
    "    \n",
    "    for i in range(len(A)):\n",
    "        lengths[i] = np.argwhere(A[i] == 0) - 1\n",
    "    \n",
    "    return length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.random.rand(2, 1, 35)\n",
    "A1 = [10, 20]\n",
    "np.divide(A/A1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_A(model, test_loader):\n",
    "\n",
    "\n",
    "    A = []\n",
    "\n",
    "    for (batch_num, collate_output) in enumerate(test_loader):\n",
    "            \n",
    "            print(\"num\", len(A))\n",
    "            with torch.autograd.set_detect_anomaly(True):\n",
    "\n",
    "                speech_input, speech_len = collate_output\n",
    "                speech_input = speech_input.to(device)\n",
    "                speech_len = torch.IntTensor(speech_len)\n",
    "                prob = 0 \n",
    "                \n",
    "                for i in range(20):\n",
    "                    \n",
    "                    prediction = model(speech_input, speech_len, train = False) \n",
    "                    pred = prediction.squeeze(0).cpu().detach().numpy()\n",
    "                  \n",
    "                    max_prob = np.amax(pred, axis = 1)                    \n",
    "                    argmax_prob = np.argmax(pred, axis = 1)\n",
    "                    length = np.argwhere(argmax_prob == 0)[0] - 1\n",
    "                    log_prob = np.log(max_prob)                  \n",
    "                    \n",
    "                    avg = (np.sum(log_prob)/length)[0]\n",
    "                    \n",
    "                    if(avg > prob):\n",
    "                        prob = avg\n",
    "                        pred = argmax_prob\n",
    "                        \n",
    "#             print(np.shape(pred))\n",
    "            A.append(pred)\n",
    "#             print(len(A))\n",
    "            \n",
    "\n",
    "    return A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_loader = torch.utils.data.DataLoader(\n",
    "        Speech2Text_valid_Dataset,\n",
    "        batch_size=1, shuffle= False, collate_fn=collate_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = test_A(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = test(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.shape(y)\n",
    "# y[0][0].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen = []\n",
    "\n",
    "for i in range(len(y)):\n",
    "    ex = y[i][0]\n",
    "    ex = ex.cpu().numpy()\n",
    "    ans =[]\n",
    "    \n",
    "    for e in ex:        \n",
    "        if(e != 0):\n",
    "            ans.append(letter_list[e - 1])        \n",
    "    chrs = \"\".join(o for o in ans[:-1])\n",
    "    sen.append(chrs)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(sen), len(ex)\n",
    "# str(sen[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('submit.csv', 'w') as fin:\n",
    "    k = 0\n",
    "    fin.write(\"Id\" + \",\" + \"Predicted\")\n",
    "    fin.write(\"\\n\")\n",
    "    for s in sen:\n",
    "        fin.write(str(k) + \",\")\n",
    "        fin.write(s)\n",
    "        fin.write(\"\\n\")\n",
    "        k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch, (frames, seq_sizes, text, _) in enumerate(dev_loader):\n",
    "    print(frames.shape)\n",
    "    print(seq_sizes)\n",
    "    print(text)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mGtabeN1ekGg"
   },
   "outputs": [],
   "source": [
    "# y = test(model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "HW4P2-aws.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
